{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "rag_dataset = load_dataset(\"neural-bridge/rag-dataset-1200\")\n",
    "\n",
    "rag_prompt = PromptTemplate.from_template(\"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\"\"\")\n",
    "\n",
    "qa_prompt = PromptTemplate.from_template(\"\"\"You are an assistant for question-answering tasks. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Answer:\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 2 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 2 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 2 failed: LLM streaming took too long.\n",
      "Attempt 3 failed: LLM streaming took too long.\n",
      "Attempt 4 failed: The model produced invalid content. Consider modifying your prompt if you are seeing this error persistently.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 2 failed: LLM streaming took too long.\n",
      "Attempt 3 failed: LLM streaming took too long.\n",
      "Attempt 4 failed: LLM streaming took too long.\n",
      "Attempt 5 failed: LLM streaming took too long.\n",
      "Max retries exceeded. Skipping.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 2 failed: LLM streaming took too long.\n",
      "Attempt 3 failed: The model produced invalid content. Consider modifying your prompt if you are seeing this error persistently.\n",
      "Attempt 4 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: The model produced invalid content. Consider modifying your prompt if you are seeing this error persistently.\n",
      "Attempt 1 failed: The model produced invalid content. Consider modifying your prompt if you are seeing this error persistently.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 2 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 2 failed: LLM streaming took too long.\n",
      "Attempt 3 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 2 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n"
     ]
    }
   ],
   "source": [
    "def run_experiment(model, prompt, context, question, temperature, num_repeats=10, alpha=0.1, max_retries=5):\n",
    "    data = []\n",
    "    llm = ChatOpenAI(model=model, temperature=temperature).bind(logprobs=True)\n",
    "    example_prompt = prompt.format(context=context, question=question) if context else prompt.format(question=question)\n",
    "    \n",
    "    for _ in range(num_repeats):\n",
    "        full = None\n",
    "        log_probs = []\n",
    "        ema_log_prob = None\n",
    "        skip = False\n",
    "\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                start_time = time.time() \n",
    "\n",
    "                for chunk in llm.stream(example_prompt):\n",
    "                    # Ensure streaming does not hang indefinitely\n",
    "                    if time.time() - start_time > 60:  # Timeout after 60 seconds\n",
    "                        raise TimeoutError(\"LLM streaming took too long.\")\n",
    "\n",
    "                    full = chunk if full is None else full + chunk\n",
    "                    if \"logprobs\" in full.response_metadata:\n",
    "                        for token in full.response_metadata[\"logprobs\"][\"content\"]:\n",
    "                            log_prob = token[\"logprob\"]\n",
    "                            log_probs.append(log_prob)\n",
    "                            ema_log_prob = alpha * log_prob + (1 - alpha) * (ema_log_prob if ema_log_prob is not None else log_prob)\n",
    "\n",
    "                break\n",
    "\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Attempt {retries} failed: {e}\")\n",
    "\n",
    "                if retries >= max_retries:\n",
    "                    print(f\"Max retries exceeded. Skipping.\")\n",
    "                    skip = True\n",
    "                    break  \n",
    "\n",
    "        if skip:\n",
    "            continue  # Skip this iteration if max retries failed\n",
    "\n",
    "        # Compute perplexities safely\n",
    "        try:\n",
    "            ppl = math.exp(-sum(log_probs) / len(log_probs)) if log_probs else None\n",
    "        except OverflowError:\n",
    "            ppl = float('inf')\n",
    "\n",
    "        try:\n",
    "            ema_ppl = math.exp(-ema_log_prob) if ema_log_prob else None\n",
    "        except OverflowError:\n",
    "            ema_ppl = float('inf')\n",
    "        \n",
    "        data.append({\n",
    "            \"Context\": context,\n",
    "            \"Question\": question,\n",
    "            \"Answer\": full.content if full else \"No response\",\n",
    "            \"Perplexity\": ppl,\n",
    "            \"EMA_Perplexity\": ema_ppl,\n",
    "            \"Temperature\": temperature,\n",
    "            \"Prompt_Type\": \"QA\" if context is None else \"RAG\"\n",
    "        })\n",
    "    \n",
    "    return data\n",
    "\n",
    "def start():\n",
    "    data = []\n",
    "    num_qa = 5 # Using first 5 examples\n",
    "    temperatures = [0, 1, 2]\n",
    "\n",
    "    for i in range(num_qa):  \n",
    "        context = rag_dataset['train'][i]['context']\n",
    "        question = rag_dataset['train'][i]['question']\n",
    "        \n",
    "        for temp in temperatures:\n",
    "            data.extend(run_experiment(\"gpt-4o-mini\", rag_prompt, context, question, temp))\n",
    "            data.extend(run_experiment(\"gpt-4o-mini\", qa_prompt, None, question, temp))  # No context case\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load existing results or start the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"results.csv\"):\n",
    "    df = pd.read_csv(\"results.csv\")\n",
    "else:\n",
    "    df = start()\n",
    "    df.to_csv(\"results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Perplexity</th>\n",
       "      <th>EMA_Perplexity</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Prompt_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Francisco Rogers found the answer to a search ...</td>\n",
       "      <td>Who found the answer to a search query collar ...</td>\n",
       "      <td>Francisco Rogers found the answer to a search ...</td>\n",
       "      <td>1.095639</td>\n",
       "      <td>1.075593</td>\n",
       "      <td>0</td>\n",
       "      <td>RAG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Francisco Rogers found the answer to a search ...</td>\n",
       "      <td>Who found the answer to a search query collar ...</td>\n",
       "      <td>Francisco Rogers found the answer to a search ...</td>\n",
       "      <td>1.103221</td>\n",
       "      <td>1.079131</td>\n",
       "      <td>0</td>\n",
       "      <td>RAG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Francisco Rogers found the answer to a search ...</td>\n",
       "      <td>Who found the answer to a search query collar ...</td>\n",
       "      <td>Francisco Rogers found the answer to a search ...</td>\n",
       "      <td>1.103234</td>\n",
       "      <td>1.078639</td>\n",
       "      <td>0</td>\n",
       "      <td>RAG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Francisco Rogers found the answer to a search ...</td>\n",
       "      <td>Who found the answer to a search query collar ...</td>\n",
       "      <td>Francisco Rogers found the answer to a search ...</td>\n",
       "      <td>1.113013</td>\n",
       "      <td>1.087556</td>\n",
       "      <td>0</td>\n",
       "      <td>RAG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Francisco Rogers found the answer to a search ...</td>\n",
       "      <td>Who found the answer to a search query collar ...</td>\n",
       "      <td>Francisco Rogers found the answer to a search ...</td>\n",
       "      <td>1.112027</td>\n",
       "      <td>1.086382</td>\n",
       "      <td>0</td>\n",
       "      <td>RAG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Context  \\\n",
       "0  Francisco Rogers found the answer to a search ...   \n",
       "1  Francisco Rogers found the answer to a search ...   \n",
       "2  Francisco Rogers found the answer to a search ...   \n",
       "3  Francisco Rogers found the answer to a search ...   \n",
       "4  Francisco Rogers found the answer to a search ...   \n",
       "\n",
       "                                            Question  \\\n",
       "0  Who found the answer to a search query collar ...   \n",
       "1  Who found the answer to a search query collar ...   \n",
       "2  Who found the answer to a search query collar ...   \n",
       "3  Who found the answer to a search query collar ...   \n",
       "4  Who found the answer to a search query collar ...   \n",
       "\n",
       "                                              Answer  Perplexity  \\\n",
       "0  Francisco Rogers found the answer to a search ...    1.095639   \n",
       "1  Francisco Rogers found the answer to a search ...    1.103221   \n",
       "2  Francisco Rogers found the answer to a search ...    1.103234   \n",
       "3  Francisco Rogers found the answer to a search ...    1.113013   \n",
       "4  Francisco Rogers found the answer to a search ...    1.112027   \n",
       "\n",
       "   EMA_Perplexity  Temperature Prompt_Type  \n",
       "0        1.075593            0         RAG  \n",
       "1        1.079131            0         RAG  \n",
       "2        1.078639            0         RAG  \n",
       "3        1.087556            0         RAG  \n",
       "4        1.086382            0         RAG  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           Perplexity                                                    EMA_Perplexity                                                     \n",
      "                                                                                 mean     median        std         min        max count           mean     median         std         min         max count\n",
      "Question                                           Temperature Prompt_Type                                                                                                                                  \n",
      "What are some of the potential negative impacts... 0           QA              1.2214     1.2315     0.0168      1.1938     1.2338    10         1.3026     1.3322      0.0805      1.2107      1.4467    10\n",
      "                                                               RAG             1.2458     1.2489     0.0143      1.2196     1.2626    10         1.3078     1.2865      0.0580      1.2525      1.3962    10\n",
      "                                                   1           QA          2.0090e+27     1.7427 6.3530e+27      1.3519 2.0090e+28    10     5.6174e+88     1.7688  1.7764e+89      1.3565  5.6174e+89    10\n",
      "                                                               RAG         7.9758e+43     1.7853 2.5222e+44      1.3573 7.9758e+44    10     9.1725e+41     1.9828  2.9006e+42      1.4865  9.1725e+42    10\n",
      "                                                   2           QA                 inf        inf        NaN         inf        inf    10            inf        inf         NaN  4.2995e+41         inf    10\n",
      "                                                               RAG                inf        inf        NaN 1.4699e+228        inf     9            inf        inf         NaN 1.2890e+263         inf     9\n",
      "What are the responsibilities of a Senior Plann... 0           QA              1.2365     1.2371     0.0039      1.2295     1.2428    10         1.4218     1.4204      0.0085      1.4082      1.4359    10\n",
      "                                                               RAG             1.0728     1.0723     0.0043      1.0682     1.0829    10         1.2407     1.2135      0.0461      1.2059      1.3145    10\n",
      "                                                   1           QA              1.4945     1.4169     0.1755      1.2868     1.7972    10         1.8327     1.7453      0.4758      1.3022      2.7217    10\n",
      "                                                               RAG             1.2122     1.1862     0.0721      1.1415     1.3392    10         1.5837     1.5951      0.2803      1.1865      1.9966    10\n",
      "                                                   2           QA                 inf        inf        NaN  1.3454e+31        inf    10            inf        inf         NaN  1.0569e+58         inf    10\n",
      "                                                               RAG                inf 3.7677e+94        NaN      1.5541        inf    10            inf 1.5126e+25         NaN      1.1897         inf    10\n",
      "What services does Pearl Moving Company in Sant... 0           QA              1.0172     1.0171     0.0000      1.0171     1.0172    10         1.0187     1.0187      0.0000      1.0187      1.0188    10\n",
      "                                                               RAG             1.1423     1.1409     0.0058      1.1350     1.1517    10         1.0871     1.0788      0.0168      1.0703      1.1080    10\n",
      "                                                   1           QA              1.0170     1.0171     0.0005      1.0160     1.0179    10         1.0187     1.0187      0.0006      1.0178      1.0201    10\n",
      "                                                               RAG             1.3287     1.2972     0.1030      1.2010     1.5535    10         1.3970     1.3563      0.2088      1.1722      1.8186    10\n",
      "                                                   2           QA              1.4296     1.0173     0.8114      1.0170     3.4852    10         1.2559     1.0187      0.4646      1.0179      2.3706    10\n",
      "                                                               RAG                inf        inf        NaN      1.7978        inf    10            inf        inf         NaN      1.4711         inf    10\n",
      "Who found the answer to a search query collar g... 0           QA              1.0468     1.0487     0.0058      1.0304     1.0487    10         1.0568     1.0590      0.0071      1.0366      1.0590    10\n",
      "                                                               RAG             1.1055     1.1035     0.0056      1.0956     1.1133    10         1.0813     1.0797      0.0043      1.0756      1.0879    10\n",
      "                                                   1           QA              1.1151     1.0487     0.1543      1.0241     1.4420    10         1.0704     1.0590      0.0430      1.0288      1.1668    10\n",
      "                                                               RAG             1.1622     1.1785     0.0686      1.0837     1.2496    10         1.1681     1.1791      0.0914      1.0735      1.3368    10\n",
      "                                                   2           QA                 inf     1.6997        NaN      1.0241        inf    10            inf     1.5231         NaN      1.0288         inf    10\n",
      "                                                               RAG                inf     1.2416        NaN      1.0719        inf    10            inf     1.3315         NaN      1.0597         inf    10\n",
      "Who were the three stars in the NHL game betwee... 0           QA              1.0149     1.0148     0.0001      1.0148     1.0150    10         1.0177     1.0176      0.0001      1.0176      1.0179    10\n",
      "                                                               RAG             1.0289     1.0298     0.0027      1.0244     1.0325    10         1.0121     1.0126      0.0019      1.0091      1.0148    10\n",
      "                                                   1           QA              1.0144     1.0149     0.0017      1.0095     1.0151    10         1.0171     1.0177      0.0021      1.0111      1.0179    10\n",
      "                                                               RAG             1.0842     1.0743     0.0591      1.0275     1.2238    10         1.1588     1.0289      0.3996      1.0107      2.2938    10\n",
      "                                                   2           QA                 inf     1.0149        NaN      1.0148        inf    10            inf     1.0177         NaN      1.0176         inf    10\n",
      "                                                               RAG                inf     1.1465        NaN      1.0325        inf    10    6.8419e+107     1.0557 1.5979e+108      1.0148 4.8795e+108    10\n"
     ]
    }
   ],
   "source": [
    "def custom_float_format(x):\n",
    "    if abs(x) > 10:\n",
    "        return '{:.4e}'.format(x)  # scientific notation for numbers > 10\n",
    "    else:\n",
    "        return '{:.4f}'.format(x)  # standard float format for numbers <= 10\n",
    "\n",
    "pd.set_option('display.max_rows', None) \n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format', custom_float_format)\n",
    "\n",
    "agg_metrics = df.groupby([\"Question\", \"Temperature\", \"Prompt_Type\"])[[\"Perplexity\", \"EMA_Perplexity\"]].agg(['mean', 'median', 'std', 'min', 'max', 'count'])\n",
    "print(agg_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median comparison\n",
    "\n",
    "By comparing **median perplexity** between RAG and QA (baseline), we assess **model predictability**. Lower perplexity indicates more confident, predictable outputs. If RAG consistently shows **lower perplexity than QA**, it suggests that the added context helps the model generate more certain responses. Conversely, higher perplexity implies that RAG may be introducing confusion or noise. This differential helps evaluate whether RAG improves or degrades response reliability compared to the standalone LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Perplexity (RAG - QA):\n",
      "Prompt_Type     QA        RAG  RAG_minus_QA\n",
      "Temperature                                \n",
      "0           1.0487     1.1035        0.0548\n",
      "1           1.0487     1.1862        0.1376\n",
      "2           1.6997 3.7677e+94    3.7677e+94\n",
      "\n",
      "Median EMA_Perplexity (RAG - QA):\n",
      "Prompt_Type     QA        RAG  RAG_minus_QA\n",
      "Temperature                                \n",
      "0           1.0590     1.0797        0.0207\n",
      "1           1.0590     1.3563        0.2973\n",
      "2           1.5231 1.5126e+25    1.5126e+25\n"
     ]
    }
   ],
   "source": [
    "# Extract and group Perplexity and EMA_Perplexity medians\n",
    "median_ppl = agg_metrics['Perplexity']['median']\n",
    "median_ema = agg_metrics['EMA_Perplexity']['median']\n",
    "\n",
    "grouped_ppl = median_ppl.groupby(['Temperature', 'Prompt_Type']).median().unstack()\n",
    "grouped_ema = median_ema.groupby(['Temperature', 'Prompt_Type']).median().unstack()\n",
    "\n",
    "# Compute the difference: RAG - QA\n",
    "grouped_ppl['RAG_minus_QA'] = grouped_ppl['RAG'] - grouped_ppl['QA']\n",
    "grouped_ema['RAG_minus_QA'] = grouped_ema['RAG'] - grouped_ema['QA']\n",
    "\n",
    "print(\"Median Perplexity (RAG - QA):\")\n",
    "print(grouped_ppl)\n",
    "\n",
    "print(\"\\nMedian EMA_Perplexity (RAG - QA):\")\n",
    "print(grouped_ema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard deviation comparison\n",
    "\n",
    "Standard deviation reflects how **consistent** the model's predictions are. A **lower std** means more stable outputs across generations. By comparing the std of RAG vs QA, we can assess if context improves or destabilizes predictability. A **negative RAG_minus_QA** value suggests RAG is **more consistent**, which is desirable for reliability in production systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Std of Perplexity (RAG - QA):\n",
      "Prompt_Type     QA    RAG  RAG_minus_QA\n",
      "Temperature                            \n",
      "0           0.0039 0.0056        0.0017\n",
      "1           0.1543 0.0721       -0.0823\n",
      "2           0.8114    NaN           NaN\n",
      "\n",
      "Mean Std of EMA_Perplexity (RAG - QA):\n",
      "Prompt_Type     QA         RAG  RAG_minus_QA\n",
      "Temperature                                 \n",
      "0           0.0071      0.0168        0.0097\n",
      "1           0.0430      0.2803        0.2373\n",
      "2           0.4646 1.5979e+108   1.5979e+108\n"
     ]
    }
   ],
   "source": [
    "# Analyze standard deviation of Perplexity and EMA_Perplexity\n",
    "std_ppl = agg_metrics['Perplexity']['std']\n",
    "std_ema = agg_metrics['EMA_Perplexity']['std']\n",
    "\n",
    "grouped_std_ppl = std_ppl.groupby(['Temperature', 'Prompt_Type']).median().unstack()\n",
    "grouped_std_ema = std_ema.groupby(['Temperature', 'Prompt_Type']).median().unstack()\n",
    "\n",
    "# Compute difference in std: RAG - QA\n",
    "grouped_std_ppl['RAG_minus_QA'] = grouped_std_ppl['RAG'] - grouped_std_ppl['QA']\n",
    "grouped_std_ema['RAG_minus_QA'] = grouped_std_ema['RAG'] - grouped_std_ema['QA']\n",
    "\n",
    "print(\"Mean Std of Perplexity (RAG - QA):\")\n",
    "print(grouped_std_ppl)\n",
    "\n",
    "print(\"\\nMean Std of EMA_Perplexity (RAG - QA):\")\n",
    "print(grouped_std_ema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_perplexity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
