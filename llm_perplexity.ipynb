{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "rag_dataset = load_dataset(\"neural-bridge/rag-dataset-1200\")\n",
    "\n",
    "rag_prompt = PromptTemplate.from_template(\"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\"\"\")\n",
    "\n",
    "qa_prompt = PromptTemplate.from_template(\"\"\"You are an assistant for question-answering tasks. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Answer:\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: The model produced invalid content. Consider modifying your prompt if you are seeing this error persistently.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 2 failed: LLM streaming took too long.\n",
      "Attempt 3 failed: LLM streaming took too long.\n",
      "Attempt 4 failed: LLM streaming took too long.\n",
      "Attempt 5 failed: LLM streaming took too long.\n",
      "Attempt 6 failed: LLM streaming took too long.\n",
      "Attempt 7 failed: LLM streaming took too long.\n",
      "Attempt 8 failed: LLM streaming took too long.\n",
      "Attempt 9 failed: LLM streaming took too long.\n",
      "Attempt 10 failed: The model produced invalid content. Consider modifying your prompt if you are seeing this error persistently.\n",
      "Max retries exceeded. Skipping.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 2 failed: The model produced invalid content. Consider modifying your prompt if you are seeing this error persistently.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 2 failed: LLM streaming took too long.\n",
      "Attempt 3 failed: LLM streaming took too long.\n",
      "Attempt 4 failed: LLM streaming took too long.\n",
      "Attempt 5 failed: The model produced invalid content. Consider modifying your prompt if you are seeing this error persistently.\n",
      "Attempt 6 failed: LLM streaming took too long.\n",
      "Attempt 7 failed: The model produced invalid content. Consider modifying your prompt if you are seeing this error persistently.\n",
      "Attempt 8 failed: LLM streaming took too long.\n",
      "Attempt 9 failed: LLM streaming took too long.\n",
      "Attempt 10 failed: LLM streaming took too long.\n",
      "Max retries exceeded. Skipping.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 2 failed: LLM streaming took too long.\n",
      "Attempt 3 failed: LLM streaming took too long.\n",
      "Attempt 4 failed: LLM streaming took too long.\n",
      "Attempt 5 failed: LLM streaming took too long.\n",
      "Attempt 6 failed: LLM streaming took too long.\n",
      "Attempt 7 failed: LLM streaming took too long.\n",
      "Attempt 8 failed: The model produced invalid content. Consider modifying your prompt if you are seeing this error persistently.\n",
      "Attempt 9 failed: LLM streaming took too long.\n",
      "Attempt 10 failed: LLM streaming took too long.\n",
      "Max retries exceeded. Skipping.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 2 failed: LLM streaming took too long.\n",
      "Attempt 3 failed: LLM streaming took too long.\n",
      "Attempt 4 failed: LLM streaming took too long.\n",
      "Attempt 5 failed: LLM streaming took too long.\n",
      "Attempt 6 failed: LLM streaming took too long.\n",
      "Attempt 7 failed: LLM streaming took too long.\n",
      "Attempt 8 failed: LLM streaming took too long.\n",
      "Attempt 9 failed: LLM streaming took too long.\n",
      "Attempt 10 failed: The model produced invalid content. Consider modifying your prompt if you are seeing this error persistently.\n",
      "Max retries exceeded. Skipping.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 2 failed: LLM streaming took too long.\n",
      "Attempt 3 failed: LLM streaming took too long.\n",
      "Attempt 4 failed: LLM streaming took too long.\n",
      "Attempt 5 failed: LLM streaming took too long.\n",
      "Attempt 6 failed: LLM streaming took too long.\n",
      "Attempt 7 failed: LLM streaming took too long.\n",
      "Attempt 8 failed: LLM streaming took too long.\n",
      "Attempt 9 failed: LLM streaming took too long.\n",
      "Attempt 10 failed: LLM streaming took too long.\n",
      "Max retries exceeded. Skipping.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 2 failed: LLM streaming took too long.\n",
      "Attempt 3 failed: LLM streaming took too long.\n",
      "Attempt 4 failed: LLM streaming took too long.\n",
      "Attempt 5 failed: The model produced invalid content. Consider modifying your prompt if you are seeing this error persistently.\n",
      "Attempt 6 failed: LLM streaming took too long.\n",
      "Attempt 7 failed: LLM streaming took too long.\n",
      "Attempt 8 failed: LLM streaming took too long.\n",
      "Attempt 9 failed: LLM streaming took too long.\n",
      "Attempt 10 failed: LLM streaming took too long.\n",
      "Max retries exceeded. Skipping.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 2 failed: LLM streaming took too long.\n",
      "Attempt 3 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 2 failed: LLM streaming took too long.\n",
      "Attempt 3 failed: LLM streaming took too long.\n",
      "Attempt 4 failed: The model produced invalid content. Consider modifying your prompt if you are seeing this error persistently.\n",
      "Attempt 5 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: The model produced invalid content. Consider modifying your prompt if you are seeing this error persistently.\n",
      "Attempt 2 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 2 failed: LLM streaming took too long.\n",
      "Attempt 3 failed: LLM streaming took too long.\n",
      "Attempt 4 failed: LLM streaming took too long.\n",
      "Attempt 5 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 2 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 2 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 2 failed: LLM streaming took too long.\n",
      "Attempt 3 failed: The model produced invalid content. Consider modifying your prompt if you are seeing this error persistently.\n",
      "Attempt 4 failed: The model produced invalid content. Consider modifying your prompt if you are seeing this error persistently.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 2 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 2 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n"
     ]
    }
   ],
   "source": [
    "def run_experiment(model, prompt, context, question, temperature, num_repeats=10, alpha=0.1, max_retries=10):\n",
    "    data = []\n",
    "    llm = ChatOpenAI(model=model, temperature=temperature).bind(logprobs=True)\n",
    "    example_prompt = prompt.format(context=context, question=question) if context else prompt.format(question=question)\n",
    "    \n",
    "    for _ in range(num_repeats):\n",
    "        full = None\n",
    "        log_probs = []\n",
    "        ema_log_prob = None\n",
    "        skip = False\n",
    "\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                start_time = time.time()  # Track start time\n",
    "\n",
    "                for chunk in llm.stream(example_prompt):\n",
    "                    # Ensure streaming does not hang indefinitely\n",
    "                    if time.time() - start_time > 15:  # Timeout after 15 seconds\n",
    "                        raise TimeoutError(\"LLM streaming took too long.\")\n",
    "\n",
    "                    full = chunk if full is None else full + chunk\n",
    "                    if \"logprobs\" in full.response_metadata:\n",
    "                        for token in full.response_metadata[\"logprobs\"][\"content\"]:\n",
    "                            log_prob = token[\"logprob\"]\n",
    "                            log_probs.append(log_prob)\n",
    "                            ema_log_prob = alpha * log_prob + (1 - alpha) * (ema_log_prob if ema_log_prob is not None else log_prob)\n",
    "\n",
    "                break  # Success, exit retry loop\n",
    "\n",
    "            except Exception as e:\n",
    "                retries += 1  # Ensure retries always increments\n",
    "                print(f\"Attempt {retries} failed: {e}\")\n",
    "\n",
    "                if retries >= max_retries:\n",
    "                    print(f\"Max retries exceeded. Skipping.\")\n",
    "                    skip = True\n",
    "                    break  # Ensure we exit retry loop\n",
    "\n",
    "        if skip:\n",
    "            continue  # Skip this iteration if max retries failed\n",
    "\n",
    "        # Compute perplexities safely\n",
    "        try:\n",
    "            ppl = math.exp(-sum(log_probs) / len(log_probs)) if log_probs else None\n",
    "        except OverflowError:\n",
    "            ppl = float('inf')\n",
    "\n",
    "        try:\n",
    "            ema_ppl = math.exp(-ema_log_prob) if ema_log_prob else None\n",
    "        except OverflowError:\n",
    "            ema_ppl = float('inf')\n",
    "        \n",
    "        data.append({\n",
    "            \"Context\": context,\n",
    "            \"Question\": question,\n",
    "            \"Answer\": full.content if full else \"No response\",\n",
    "            \"Perplexity\": ppl,\n",
    "            \"EMA_Perplexity\": ema_ppl,\n",
    "            \"Temperature\": temperature,\n",
    "            \"Prompt_Type\": \"QA\" if context is None else \"RAG\"\n",
    "        })\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = []\n",
    "num_qa = 5 # Using first 5 examples\n",
    "temperatures = [0, 1, 2]\n",
    "\n",
    "for i in range(num_qa):  \n",
    "    context = rag_dataset['train'][i]['context']\n",
    "    question = rag_dataset['train'][i]['question']\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        data.extend(run_experiment(\"gpt-4o-mini\", rag_prompt, context, question, temp))\n",
    "        data.extend(run_experiment(\"gpt-4o-mini\", qa_prompt, None, question, temp))  # No context case\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Perplexity</th>\n",
       "      <th>EMA_Perplexity</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Prompt_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Francisco Rogers found the answer to a search ...</td>\n",
       "      <td>Who found the answer to a search query collar ...</td>\n",
       "      <td>Francisco Rogers found the answer to a search ...</td>\n",
       "      <td>1.1141</td>\n",
       "      <td>1.0902</td>\n",
       "      <td>0</td>\n",
       "      <td>RAG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Francisco Rogers found the answer to a search ...</td>\n",
       "      <td>Who found the answer to a search query collar ...</td>\n",
       "      <td>Francisco Rogers found the answer to a search ...</td>\n",
       "      <td>1.1049</td>\n",
       "      <td>1.0814</td>\n",
       "      <td>0</td>\n",
       "      <td>RAG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Francisco Rogers found the answer to a search ...</td>\n",
       "      <td>Who found the answer to a search query collar ...</td>\n",
       "      <td>Francisco Rogers found the answer to the searc...</td>\n",
       "      <td>1.0808</td>\n",
       "      <td>1.0743</td>\n",
       "      <td>0</td>\n",
       "      <td>RAG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Francisco Rogers found the answer to a search ...</td>\n",
       "      <td>Who found the answer to a search query collar ...</td>\n",
       "      <td>Francisco Rogers found the answer to the searc...</td>\n",
       "      <td>1.0673</td>\n",
       "      <td>1.0584</td>\n",
       "      <td>0</td>\n",
       "      <td>RAG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Francisco Rogers found the answer to a search ...</td>\n",
       "      <td>Who found the answer to a search query collar ...</td>\n",
       "      <td>Francisco Rogers found the answer to the searc...</td>\n",
       "      <td>1.0802</td>\n",
       "      <td>1.0739</td>\n",
       "      <td>0</td>\n",
       "      <td>RAG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Context                                           Question                                             Answer  Perplexity  EMA_Perplexity  Temperature Prompt_Type\n",
       "0  Francisco Rogers found the answer to a search ...  Who found the answer to a search query collar ...  Francisco Rogers found the answer to a search ...      1.1141          1.0902            0         RAG\n",
       "1  Francisco Rogers found the answer to a search ...  Who found the answer to a search query collar ...  Francisco Rogers found the answer to a search ...      1.1049          1.0814            0         RAG\n",
       "2  Francisco Rogers found the answer to a search ...  Who found the answer to a search query collar ...  Francisco Rogers found the answer to the searc...      1.0808          1.0743            0         RAG\n",
       "3  Francisco Rogers found the answer to a search ...  Who found the answer to a search query collar ...  Francisco Rogers found the answer to the searc...      1.0673          1.0584            0         RAG\n",
       "4  Francisco Rogers found the answer to a search ...  Who found the answer to a search query collar ...  Francisco Rogers found the answer to the searc...      1.0802          1.0739            0         RAG"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           Perplexity                                                     EMA_Perplexity                                                     \n",
      "                                                                                 mean        std         min        max      median count           mean         std        min         max      median count\n",
      "Question                                           Temperature Prompt_Type                                                                                                                                   \n",
      "What are some of the potential negative impacts... 0           QA              1.2242     0.0132      1.1967     1.2340      1.2292    10         1.2943      0.0646     1.2121      1.3565      1.3333    10\n",
      "                                                               RAG             1.2517     0.0115      1.2210     1.2617      1.2547    10         1.2696      0.0486     1.2468      1.4066      1.2538    10\n",
      "                                                   1           QA          1.8033e+82 5.7025e+82      1.3064 1.8033e+83      1.5495    10     2.7653e+24  8.7447e+24     1.4194  2.7653e+25      2.0756    10\n",
      "                                                               RAG         2.0898e+31 6.6074e+31      1.5920 2.0895e+32      1.8054    10     5.6181e+64  1.7766e+65     1.5955  5.6181e+65      1.9752    10\n",
      "                                                   2           QA                 inf        NaN 1.3576e+111        inf         inf    10            inf         NaN 4.6270e+73         inf         inf    10\n",
      "                                                               RAG                inf        NaN 2.8597e+199        inf         inf     4            inf         NaN 1.4554e+35         inf         inf     4\n",
      "What are the responsibilities of a Senior Plann... 0           QA              1.2333     0.0043      1.2259     1.2400      1.2340    10         1.3795      0.0936     1.2218      1.4477      1.4296    10\n",
      "                                                               RAG             1.0765     0.0035      1.0720     1.0812      1.0763    10         1.2806      0.0394     1.2286      1.3225      1.3016    10\n",
      "                                                   1           QA          1.9705e+41 6.2312e+41      1.3179 1.9705e+42      1.4994    10     8.9645e+98  2.8348e+99     1.4930  8.9645e+99      1.8382    10\n",
      "                                                               RAG             1.1963     0.0525      1.1320     1.2753      1.1797    10         1.4355      0.2711     1.1366      2.0295      1.3789    10\n",
      "                                                   2           QA                 inf        NaN      2.2773        inf 1.0067e+278    10            inf         NaN     2.3649         inf         inf    10\n",
      "                                                               RAG                inf        NaN      1.4433        inf  1.2908e+78    10            inf         NaN     1.6074         inf 3.0344e+100    10\n",
      "What services does Pearl Moving Company in Sant... 0           QA              1.0179     0.0013      1.0157     1.0206      1.0181    10         1.0183      0.0011     1.0166      1.0207      1.0184    10\n",
      "                                                               RAG             1.1485     0.0066      1.1364     1.1537      1.1512    10         1.0969      0.0116     1.0799      1.1049      1.1035    10\n",
      "                                                   1           QA              1.2675     0.7919      1.0147     3.5213      1.0181    10         1.1694      0.4798     1.0160      2.5349      1.0184    10\n",
      "                                                               RAG             1.3586     0.1473      1.1826     1.7222      1.3296    10         1.4224      0.3496     1.0763      2.2377      1.3142    10\n",
      "                                                   2           QA                 inf        NaN      1.0172        inf      1.0191    10            inf         NaN     1.0180         inf      1.0200    10\n",
      "                                                               RAG                inf        NaN      1.5805        inf 7.2820e+148    10            inf         NaN     2.6732         inf  8.4543e+28    10\n",
      "Who found the answer to a search query collar g... 0           QA              1.0246     0.0131      1.0192     1.0613      1.0192    10         1.0293      0.0161     1.0228      1.0746      1.0229    10\n",
      "                                                               RAG             1.0886     0.0157      1.0673     1.1141      1.0815    10         1.0749      0.0086     1.0584      1.0902      1.0750    10\n",
      "                                                   1           QA              1.0218     0.0026      1.0192     1.0248      1.0217    10         1.0259      0.0032     1.0228      1.0294      1.0259    10\n",
      "                                                               RAG             1.1572     0.1160      1.0672     1.3859      1.0966    10         1.2078      0.2270     1.0583      1.6584      1.0791    10\n",
      "                                                   2           QA                 inf        NaN      1.0192        inf      2.8455    10            inf         NaN     1.0228         inf      2.4244    10\n",
      "                                                               RAG             1.7659     1.4992      1.0745     5.9793      1.2141    10         1.5433      0.6407     1.0636      3.1403      1.2769    10\n",
      "Who were the three stars in the NHL game betwee... 0           QA              1.0150     0.0012      1.0117     1.0155      1.0154    10         1.0178      0.0014     1.0139      1.0184      1.0183    10\n",
      "                                                               RAG             1.0296     0.0016      1.0275     1.0330      1.0298    10         1.0123      0.0013     1.0107      1.0149      1.0126    10\n",
      "                                                   1           QA              1.1195     0.3312      1.0117     2.0621      1.0153    10         1.0963      0.2491     1.0138      1.8052      1.0181    10\n",
      "                                                               RAG             1.0647     0.0382      1.0270     1.1126      1.0433    10         1.0241      0.0116     1.0106      1.0477      1.0259    10\n",
      "                                                   2           QA                 inf        NaN      1.0150        inf      1.0154    10    1.5376e+120 4.8623e+120     1.0178 1.5376e+121      1.0183    10\n",
      "                                                               RAG                inf        NaN      1.0280        inf      1.3425    10     5.6767e+46  1.7951e+47     1.0109  5.6767e+47      1.0731    10\n"
     ]
    }
   ],
   "source": [
    "def custom_float_format(x):\n",
    "    if abs(x) > 10:\n",
    "        return '{:.4e}'.format(x)  # scientific notation for numbers > 10\n",
    "    else:\n",
    "        return '{:.4f}'.format(x)  # standard float format for numbers <= 10\n",
    "\n",
    "pd.set_option('display.max_rows', None) \n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format', custom_float_format)\n",
    "\n",
    "agg_metrics = df.groupby([\"Question\", \"Temperature\", \"Prompt_Type\"])[[\"Perplexity\", \"EMA_Perplexity\"]].agg(['mean', 'std', 'min', 'max', 'median', 'count'])\n",
    "print(agg_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_perplexity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
