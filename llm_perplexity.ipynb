{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "rag_dataset = load_dataset(\"neural-bridge/rag-dataset-1200\")\n",
    "\n",
    "rag_prompt = PromptTemplate.from_template(\"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\"\"\")\n",
    "\n",
    "qa_prompt = PromptTemplate.from_template(\"\"\"You are an assistant for question-answering tasks. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Answer:\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 2 failed: LLM streaming took too long.\n",
      "Attempt 3 failed: LLM streaming took too long.\n",
      "Attempt 4 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 2 failed: The model produced invalid content. Consider modifying your prompt if you are seeing this error persistently.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 2 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 2 failed: LLM streaming took too long.\n",
      "Attempt 3 failed: LLM streaming took too long.\n",
      "Attempt 4 failed: LLM streaming took too long.\n",
      "Attempt 5 failed: LLM streaming took too long.\n",
      "Max retries exceeded. Skipping.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 2 failed: The model produced invalid content. Consider modifying your prompt if you are seeing this error persistently.\n",
      "Attempt 3 failed: LLM streaming took too long.\n",
      "Attempt 4 failed: LLM streaming took too long.\n",
      "Attempt 5 failed: LLM streaming took too long.\n",
      "Max retries exceeded. Skipping.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 2 failed: LLM streaming took too long.\n",
      "Attempt 3 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 2 failed: The model produced invalid content. Consider modifying your prompt if you are seeing this error persistently.\n",
      "Attempt 1 failed: The model produced invalid content. Consider modifying your prompt if you are seeing this error persistently.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 2 failed: The model produced invalid content. Consider modifying your prompt if you are seeing this error persistently.\n",
      "Attempt 1 failed: The server had an error while processing your request. Sorry about that!\n",
      "Attempt 1 failed: The model produced invalid content. Consider modifying your prompt if you are seeing this error persistently.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 2 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 1 failed: LLM streaming took too long.\n",
      "Attempt 2 failed: The model produced invalid content. Consider modifying your prompt if you are seeing this error persistently.\n",
      "Attempt 1 failed: The model produced invalid content. Consider modifying your prompt if you are seeing this error persistently.\n"
     ]
    }
   ],
   "source": [
    "def run_experiment(model, prompt, context, question, temperature, num_repeats=10, alpha=0.1, max_retries=5):\n",
    "    data = []\n",
    "    llm = ChatOpenAI(model=model, temperature=temperature).bind(logprobs=True)\n",
    "    example_prompt = prompt.format(context=context, question=question) if context else prompt.format(question=question)\n",
    "    \n",
    "    for _ in range(num_repeats):\n",
    "        full = None\n",
    "        log_probs = []\n",
    "        ema_log_prob = None\n",
    "        skip = False\n",
    "\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                start_time = time.time()  # Track start time\n",
    "\n",
    "                for chunk in llm.stream(example_prompt):\n",
    "                    # Ensure streaming does not hang indefinitely\n",
    "                    if time.time() - start_time > 60:  # Timeout after 60 seconds\n",
    "                        raise TimeoutError(\"LLM streaming took too long.\")\n",
    "\n",
    "                    full = chunk if full is None else full + chunk\n",
    "                    if \"logprobs\" in full.response_metadata:\n",
    "                        for token in full.response_metadata[\"logprobs\"][\"content\"]:\n",
    "                            log_prob = token[\"logprob\"]\n",
    "                            log_probs.append(log_prob)\n",
    "                            ema_log_prob = alpha * log_prob + (1 - alpha) * (ema_log_prob if ema_log_prob is not None else log_prob)\n",
    "\n",
    "                break  # Success, exit retry loop\n",
    "\n",
    "            except Exception as e:\n",
    "                retries += 1  # Ensure retries always increments\n",
    "                print(f\"Attempt {retries} failed: {e}\")\n",
    "\n",
    "                if retries >= max_retries:\n",
    "                    print(f\"Max retries exceeded. Skipping.\")\n",
    "                    skip = True\n",
    "                    break  # Ensure we exit retry loop\n",
    "\n",
    "        if skip:\n",
    "            continue  # Skip this iteration if max retries failed\n",
    "\n",
    "        # Compute perplexities safely\n",
    "        try:\n",
    "            ppl = math.exp(-sum(log_probs) / len(log_probs)) if log_probs else None\n",
    "        except OverflowError:\n",
    "            ppl = float('inf')\n",
    "\n",
    "        try:\n",
    "            ema_ppl = math.exp(-ema_log_prob) if ema_log_prob else None\n",
    "        except OverflowError:\n",
    "            ema_ppl = float('inf')\n",
    "        \n",
    "        data.append({\n",
    "            \"Context\": context,\n",
    "            \"Question\": question,\n",
    "            \"Answer\": full.content if full else \"No response\",\n",
    "            \"Perplexity\": ppl,\n",
    "            \"EMA_Perplexity\": ema_ppl,\n",
    "            \"Temperature\": temperature,\n",
    "            \"Prompt_Type\": \"QA\" if context is None else \"RAG\"\n",
    "        })\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = []\n",
    "num_qa = 5 # Using first 5 examples\n",
    "temperatures = [0, 1, 2]\n",
    "\n",
    "for i in range(num_qa):  \n",
    "    context = rag_dataset['train'][i]['context']\n",
    "    question = rag_dataset['train'][i]['question']\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        data.extend(run_experiment(\"gpt-4o-mini\", rag_prompt, context, question, temp))\n",
    "        data.extend(run_experiment(\"gpt-4o-mini\", qa_prompt, None, question, temp))  # No context case\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Perplexity</th>\n",
       "      <th>EMA_Perplexity</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Prompt_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Francisco Rogers found the answer to a search ...</td>\n",
       "      <td>Who found the answer to a search query collar ...</td>\n",
       "      <td>Francisco Rogers found the answer to a search ...</td>\n",
       "      <td>1.1050</td>\n",
       "      <td>1.0804</td>\n",
       "      <td>0</td>\n",
       "      <td>RAG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Francisco Rogers found the answer to a search ...</td>\n",
       "      <td>Who found the answer to a search query collar ...</td>\n",
       "      <td>Francisco Rogers found the answer to the searc...</td>\n",
       "      <td>1.0673</td>\n",
       "      <td>1.0584</td>\n",
       "      <td>0</td>\n",
       "      <td>RAG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Francisco Rogers found the answer to a search ...</td>\n",
       "      <td>Who found the answer to a search query collar ...</td>\n",
       "      <td>Francisco Rogers found the answer to the searc...</td>\n",
       "      <td>1.0770</td>\n",
       "      <td>1.0699</td>\n",
       "      <td>0</td>\n",
       "      <td>RAG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Francisco Rogers found the answer to a search ...</td>\n",
       "      <td>Who found the answer to a search query collar ...</td>\n",
       "      <td>Francisco Rogers found the answer to the searc...</td>\n",
       "      <td>1.0773</td>\n",
       "      <td>1.0705</td>\n",
       "      <td>0</td>\n",
       "      <td>RAG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Francisco Rogers found the answer to a search ...</td>\n",
       "      <td>Who found the answer to a search query collar ...</td>\n",
       "      <td>Francisco Rogers found the answer to the searc...</td>\n",
       "      <td>1.0750</td>\n",
       "      <td>1.0675</td>\n",
       "      <td>0</td>\n",
       "      <td>RAG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Context                                           Question                                             Answer  Perplexity  EMA_Perplexity  Temperature Prompt_Type\n",
       "0  Francisco Rogers found the answer to a search ...  Who found the answer to a search query collar ...  Francisco Rogers found the answer to a search ...      1.1050          1.0804            0         RAG\n",
       "1  Francisco Rogers found the answer to a search ...  Who found the answer to a search query collar ...  Francisco Rogers found the answer to the searc...      1.0673          1.0584            0         RAG\n",
       "2  Francisco Rogers found the answer to a search ...  Who found the answer to a search query collar ...  Francisco Rogers found the answer to the searc...      1.0770          1.0699            0         RAG\n",
       "3  Francisco Rogers found the answer to a search ...  Who found the answer to a search query collar ...  Francisco Rogers found the answer to the searc...      1.0773          1.0705            0         RAG\n",
       "4  Francisco Rogers found the answer to a search ...  Who found the answer to a search query collar ...  Francisco Rogers found the answer to the searc...      1.0750          1.0675            0         RAG"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            Perplexity                                                      EMA_Perplexity                                                   \n",
      "                                                                                  mean      median         std        min         max count           mean      median        std        min        max count\n",
      "Question                                           Temperature Prompt_Type                                                                                                                                   \n",
      "What are some of the potential negative impacts... 0           QA               1.2279      1.2380      0.0198     1.1953      1.2479    10         1.2695      1.2318     0.0650     1.2124     1.3623    10\n",
      "                                                               RAG              1.2457      1.2555      0.0177     1.2182      1.2609    10         1.2951      1.2638     0.0565     1.2513     1.3816    10\n",
      "                                                   1           QA           8.3016e+44      1.4647  2.6252e+45     1.2843  8.3016e+45    10     1.1479e+42      1.9301 3.6301e+42     1.2149 1.1479e+43    10\n",
      "                                                               RAG         1.2851e+118      1.7728 4.0638e+118     1.5059 1.2851e+119    10     1.6078e+44      1.7064 5.0845e+44     1.5115 1.6078e+45    10\n",
      "                                                   2           QA                  inf         inf         NaN 9.3236e+41         inf    10            inf         inf        NaN 9.5483e+89        inf    10\n",
      "                                                               RAG                 inf         inf         NaN        inf         inf     8            inf         inf        NaN        inf        inf     8\n",
      "What are the responsibilities of a Senior Plann... 0           QA               1.2346      1.2353      0.0058     1.2255      1.2432    10         1.4109      1.4333     0.0716     1.2100     1.4460    10\n",
      "                                                               RAG              1.0769      1.0769      0.0033     1.0723      1.0822    10         1.2853      1.2967     0.0314     1.2362     1.3171    10\n",
      "                                                   1           QA               1.5826      1.5585      0.1260     1.3739      1.7701    10         1.7604      1.7984     0.1739     1.4512     1.9583    10\n",
      "                                                               RAG              1.2365      1.2211      0.0801     1.1289      1.3485    10         1.5349      1.5563     0.1911     1.2542     1.7928    10\n",
      "                                                   2           QA                  inf 2.8700e+303         NaN     2.7110         inf    10            inf 3.4375e+205        NaN     4.6416        inf    10\n",
      "                                                               RAG                 inf  3.6127e+25         NaN     1.4634         inf    10            inf  1.9824e+38        NaN     1.8827        inf    10\n",
      "What services does Pearl Moving Company in Sant... 0           QA               1.0190      1.0181      0.0029     1.0157      1.0251    10         1.0195      1.0184     0.0029     1.0166     1.0259    10\n",
      "                                                               RAG              1.1508      1.1510      0.0038     1.1435      1.1556    10         1.0989      1.1039     0.0113     1.0775     1.1115    10\n",
      "                                                   1           QA               1.0447      1.0181      0.0817     1.0120      1.2769    10         1.0242      1.0188     0.0153     1.0130     1.0664    10\n",
      "                                                               RAG              1.3136      1.3121      0.0576     1.1907      1.3929    10         1.5918      1.5299     0.3023     1.1250     2.0789    10\n",
      "                                                   2           QA          4.8488e+240      1.0181         inf     1.0164 4.8488e+241    10     7.5562e+86      1.0184 2.3895e+87     1.0180 7.5562e+87    10\n",
      "                                                               RAG                 inf 9.7745e+166         NaN     2.2776         inf    10            inf 5.9855e+122        NaN     1.8962        inf    10\n",
      "Who found the answer to a search query collar g... 0           QA               1.0202      1.0192      0.0021     1.0192      1.0242    10         1.0241      1.0229     0.0025     1.0228     1.0289    10\n",
      "                                                               RAG              1.0795      1.0772      0.0129     1.0670      1.1050    10         1.0685      1.0702     0.0076     1.0581     1.0804    10\n",
      "                                                   1           QA               1.0250      1.0192      0.0130     1.0192      1.0615    10         1.0299      1.0229     0.0160     1.0228     1.0748    10\n",
      "                                                               RAG              1.1762      1.2078      0.0637     1.0741      1.2443    10         1.2962      1.2427     0.2077     1.0622     1.5891    10\n",
      "                                                   2           QA                  inf      4.0089         NaN     1.0192         inf    10            inf  1.3916e+01        NaN     1.0228        inf    10\n",
      "                                                               RAG              1.4923      1.2636      0.5426     1.0526      2.8529    10         2.1769      1.3067     1.7585     1.0481     6.6297    10\n",
      "Who were the three stars in the NHL game betwee... 0           QA               1.0153      1.0154      0.0002     1.0148      1.0155    10         1.0182      1.0183     0.0003     1.0176     1.0184    10\n",
      "                                                               RAG              1.0288      1.0287      0.0020     1.0250      1.0324    10         1.0117      1.0117     0.0016     1.0090     1.0147    10\n",
      "                                                   1           QA               1.0144      1.0152      0.0019     1.0094      1.0154    10         1.0171      1.0180     0.0024     1.0110     1.0183    10\n",
      "                                                               RAG              1.0552      1.0325      0.0476     1.0275      1.1698    10         1.0218      1.0148     0.0131     1.0107     1.0476    10\n",
      "                                                   2           QA               1.3141      1.0154      0.9467     1.0094      4.0083    10         1.1907      1.0183     0.5478     1.0110     2.7498    10\n",
      "                                                               RAG                 inf      1.1500         NaN     1.0275         inf    10            inf      1.0499        NaN     1.0107        inf    10\n"
     ]
    }
   ],
   "source": [
    "def custom_float_format(x):\n",
    "    if abs(x) > 10:\n",
    "        return '{:.4e}'.format(x)  # scientific notation for numbers > 10\n",
    "    else:\n",
    "        return '{:.4f}'.format(x)  # standard float format for numbers <= 10\n",
    "\n",
    "pd.set_option('display.max_rows', None) \n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format', custom_float_format)\n",
    "\n",
    "agg_metrics = df.groupby([\"Question\", \"Temperature\", \"Prompt_Type\"])[[\"Perplexity\", \"EMA_Perplexity\"]].agg(['mean', 'median', 'std', 'min', 'max', 'count'])\n",
    "print(agg_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_perplexity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
